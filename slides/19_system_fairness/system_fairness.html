<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>MLiP: Improving Fairness</title>
    <link rel="shortcut icon" href="./../favicon.ico" />
    <link rel="stylesheet" href="./../dist/reset.css" />
    <link rel="stylesheet" href="./../dist/reveal.css" />
    <link rel="stylesheet" href="./../dist/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./../css/highlight/base16/zenburn.css" />
    <link rel="stylesheet" href="./../_assets/cmu.css" />

    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Lato" />

    <!-- <link rel="stylesheet" href="./../css/print/paper.css" type="text/css" media="print" /> -->

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><div id="footer">Machine Learning in Production/AI Engineering ‚Ä¢ Christian Kaestner & Sherry Wu, Carnegie Mellon University ‚Ä¢ Fall 2024</div><section  data-markdown><script type="text/template">
<!-- .element: class="titleslide"  data-background="../_chapterimg/17_fairnessgame.jpg" -->
<div class="stretch"></div>

## Machine Learning in Production


# Building Fair Products


</script></section><section ><section data-markdown><script type="text/template"># Administrativa

[Homework I4](https://github.com/mlip-cmu/f2024/blob/main/assignments/I4_explainability.md) to be released next week; 1 week assignment, due Nov 25

<!-- colstart -->

![aeye health commercial tool](https://www.optomed.com/wp-content/uploads/2020/09/News-postsimage.jpg)

<!-- col -->

![diabetic retinopathy, eye photo](diabetic_retinopathy.png)

<!-- col -->

![smartphone with lense attached](diabetic_retinopathy_lens.jpg)

<!-- colend -->
</script></section><section data-markdown><script type="text/template">## Research in this Course (in I4)

<div class="small">

> We are conducting academic research on explainability policies and evidence. This research will involve analyzing student work of this assignment. You will not be asked to do anything above and beyond the normal learning activities and assignments that are part of this course. You are free not to participate in this research, and your participation will have no influence on your grade for this course or your academic career at CMU. If you do not wish to participate, please send an email to Nadia Nahar (nadian@andrew.cmu.edu). Participants will not receive any compensation or extra credit. The data collected as part of this research will not include student grades. All analyses of data from participants‚Äô coursework will be conducted after the course is over and final grades are submitted -- instructors do not know who choses not to participate before final grades are submitted. All data will be analyzed in de-identified form and presented in the aggregate, without any personal identifiers. The de-identified homework solutions (without grades) will be shared with research collaborators at Yale university for further analysis. If you have questions pertaining to your rights as a research participant, or to report concerns to this study, please contact Nadia Nahar (nadian@andrew.cmu.edu) or the Office of Research Integrity and Compliance at Carnegie Mellon University (irb-review@andrew.cmu.edu; phone: 412-268-4721).

</div>
</script></section></section><section ><section data-markdown><script type="text/template">## From Fairness Concepts to Fair Products

![Overview of course content](../_assets/overview.svg)
<!-- .element: class="plain stretch" -->


</script></section><section data-markdown><script type="text/template">## Reading

Required reading: 
* Holstein, Kenneth, Jennifer Wortman Vaughan, Hal
Daum√© III, Miro Dudik, and Hanna
Wallach. "[Improving fairness in machine learning systems: What do industry practitioners need?](http://users.umiacs.umd.edu/~hal/docs/daume19fairness.pdf)"
In Proceedings of the 2019 CHI Conference on Human Factors in
Computing Systems, pp. 1-16. 2019. 

Recommended reading:
* Metcalf, Jacob, and Emanuel Moss. "[Owning ethics: Corporate logics, silicon valley, and the institutionalization of ethics](https://datasociety.net/wp-content/uploads/2019/09/Owning-Ethics-PDF-version-2.pdf)." *Social Research: An International Quarterly* 86, no. 2 (2019): 449-476.
</script></section><section data-markdown><script type="text/template">## Learning Goals

* Understand the role of requirements engineering in selecting ML
fairness criteria
* Understand the process of constructing datasets for fairness
* Document models and datasets to communicate fairness concerns
* Consider the potential impact of feedback loops on AI-based systems
  and need for continuous monitoring
* Consider achieving fairness in AI-based systems as an activity throughout the entire development cycle
</script></section></section><section ><section data-markdown><script type="text/template">## Today: Fairness as a System Quality

Fairness can be measured for a model

... but we really care whether the system, as it interacts with the environment, is fair/safe/secure

... does the system cause harm?

![System thinking](component.svg)
<!-- .element: class="plain stretch" -->
</script></section><section data-markdown><script type="text/template">## Fair ML Pipeline Process

Fairness must be considered throughout the entire lifecycle!

![](fairness-lifecycle.jpg)
<!-- .element: class="stretch" -->

<!-- references_ -->

_Fairness-aware Machine Learning_, Bennett et al., WSDM Tutorial (2019).

</script></section><section data-markdown><script type="text/template">## Fairness Problems are System-Wide Challenges

* **Requirements engineering challenges:** How to identify fairness concerns, fairness metric, design data collection and labeling
* **Human-computer-interaction design challenges:** How to present results to users, fairly collect data from users, design mitigations
* **Quality assurance challenges:** Evaluate the entire system for fairness, continuously assure in production
* **Process integration challenges:** Incorprorate fairness work in development process
* **Education and documentation challenges:** Create awareness, foster interdisciplinary collaboration

</script></section></section><section ><section data-markdown><script type="text/template"># Understanding System-Level Goals for Fairness

i.e., Requirements engineering

</script></section><section data-markdown><script type="text/template">
## Recall: Fairness metrics

* Anti-classification (fairness through blindness)
* Group fairness (independence)
* Equalized odds (separation)
* ...and numerous others and variations!

**But which one makes most sense for my product?**
</script></section><section data-markdown><script type="text/template">## Recall Breakout: Cancer Prognosis

![](cancer-stats.png)

* Does the model meet anti-classification fairness? _Can't tell_
* Does the model meet group fairness? _Sure_
   * P[cancer, M] = 0.034, P[cancer, F] = 0.036
* Does the model meet equalized odds? _No_
   * FPR, M = 0.012, F = 0.010
   * FNR, M = 0.64,  F = 0.133 
* Is the model fair enough to use?  How can we decide?

<aside class="notes"><p>prob cancer male vs female</p>
</aside></script></section><section data-markdown><script type="text/template">## Different "fairness" are...not compatible

As we will see, different fairness metrics reflect different fairness goals, different user groups that we care, and different trade-offs.

<div class="smallish">

* **Equalized odds -- minimize disparate treatment.** "We should just focus on making the right decisions; We are fair by treating everybody equally and making sure everyone gets the opportunity they actually deserve."  (equality)
* **Group fairness -- minimize disparate impact.** "We should carefully massage our decisions (and maybe be more lenient with giving one group more opportunity) to make sure that we give all the groups the same final outcomes." (equity)

</div>
</script></section><section data-markdown><script type="text/template">## Identifying Fairness Goals is a Requirements Engineering Problem

<div class="smallish">

* What is the goal of the system? What benefits does it provide and to
  whom?
  <!-- .element: class="fragment" -->
* Who are the stakeholders of the system? What are the stakeholders‚Äô views or expectations on fairness and where do they conflict? Are we trying to achieve fairness based on equality or equity? 
<!-- .element: class="fragment" -->
* What subpopulations (including minority groups) may be using or be affected by the system? What types of harms can the system cause with discrimination?
<!-- .element: class="fragment" -->
* Does fairness undermine any other goals of the system (e.g., accuracy, profits, time to release)?
<!-- .element: class="fragment" -->
* Are there legal anti-discrimination requirements to consider? Are
  there societal expectations about ethics w.r.t. to this product? What is the activist position?
<!-- .element: class="fragment" -->
* ...
<!-- .element: class="fragment" -->

</div>

</script></section><section data-markdown><script type="text/template">## 1. Identify Protected Attributes

Against which groups might we discriminate? What attributes identify them directly or indirectly?
**Requires understanding of target population and subpopulations!**

</script></section><section data-markdown><script type="text/template">## Protected attributes are not always obvious

![ATM](atm.gif)
<!-- .element: class="stretch" -->

**Q. Other examples?**

</script></section><section data-markdown><script type="text/template">## 1. Identify Protected Attributes

Against which groups might we discriminate? What attributes identify them directly or indirectly?
**Requires understanding of target population and subpopulations!**

* Use anti-discrimination law as starting point, but do not end there
  - Socio-economic status? Body height? Weight? Hair style? Eye color? Sports team preferences?
  - Protected attributes for non-humans? Animals, inanimate objects?
* Involve stakeholders, consult lawyers, read research, ask experts, ...


</script></section><section data-markdown><script type="text/template">## 2. Analyze Potential Harms

1. **Anticipate harms from unfair decisions**
  - Harms of allocation, harms of representation?
  - How do biased model predictions contribute to system behavior?

2. **Consider how automation can amplify harm**

3. **Overcome blind spots within teams**
  - Systematically consider consequences of bias
  - Consider safety engineering techniques (e.g., FTA)
  - Assemble diverse teams, use personas, crowdsource audits

</script></section><section data-markdown><script type="text/template">## Example: Judgment Call Game

<!-- colstart -->

Card "Game" by Microsoft Research

Participants write "Product reviews" from different perspectives
* encourage thinking about consequences
* enforce persona-like role taking

<!-- col -->

![Photo of Judgment Call Game cards](../_chapterimg/17_fairnessgame.jpg)
<!-- .element: class="stretch" -->

<!-- colend -->
</script></section><section data-markdown><script type="text/template">## Example: Judgment Call Game

<!-- colstart -->
![user-review1](user-review1.png)

<!-- col -->
![user-review2](user-review2.png)
<!-- .element: class="stretch" -->

<!-- colend -->

<!--references_-->
[Judgment Call the Game: Using Value Sensitive Design and Design
Fiction to Surface Ethical Concerns Related to Technology](https://dl.acm.org/doi/10.1145/3322276.3323697)
</script></section><section data-markdown><script type="text/template">## 3. Negotiate Fairness Goals/Measures

* Negotiate with stakeholders to determine fairness requirement for
the product: What is the suitable notion of fairness for the
product? Equality or equity?
* Map the requirements to model-level  (model) specifications: Anti-classification? Group fairness? Equalized odds? 
* Negotiation can be challenging! 
  * Conflicts with other system goals (accuracy, profits...) 
  * Conflicts among different beliefs, values, political views, etc., 
</script></section><section data-markdown><script type="text/template">## Intuitive Justice: Research on what people perceive as fair (psychology)

<div class="smallish">

* When rewards depend on inputs and participants can chose contributions: Most people find it fair to split rewards proportional to inputs
* Most people agree that for a decision to be fair, personal characteristics that do not influence the reward, such as gender or age, should not be considered when dividing the rewards. 

Complexity: Individual and group differences not always clearly attributable, unequal starting positions

</div>
</script></section><section data-markdown><script type="text/template">## Dealing with unequal starting positions

<div class="smallish">

**Equality (minimize disparate treatment):**
* Treat everybody equally, regardless of starting position
* Focus on meritocracy, strive for fair opportunities
* Equalized-odds-style fairness; equality of opportunity

**Equity (minimize disparate impact):**
* Lift disadvantaged group, affirmative action
* Strive for similar outcomes (distributive justice)
* Group-fairness-style fairness; equality of outcomes

Each rooted in long history of law/philosophy, and typically incompatible. 
Problem and goal dependent

</div>
</script></section><section data-markdown><script type="text/template">## Heuristic: Punitive vs Assistive Decisions

* If the decision is **punitive** in nature:
  * Harm is caused when a group is given an unwarranted penalty
  * e.g. decide whom to deny bail based on risk of recidivism
  * Heuristic: Use a fairness metric (equalized odds) based on **FPR**
* If the decision is **assistive** in nature:
  * Harm is caused when a group in need is denied assistance
  * e.g., decide who should receive a loan or a food subsidy
  * Heuristic: Use a fairness metric based on **FNR**
</script></section><section data-markdown><script type="text/template">## Fairness Tree

![](fairness_tree.png)
<!-- .element: class="stretch plain" -->

<!-- references_ -->

Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter and Julia Lane. [Big Data and Social Science: Data Science Methods and Tools for Research and Practice](https://textbook.coleridgeinitiative.org/). Chapter 11, 2nd ed, 2020

</script></section><section data-markdown><script type="text/template">## Trade-offs in Fairness vs Accuracy

<!-- colstart -->

![](fairness-accuracy.jpeg)
<!-- .element: class="stretch" -->


<!-- col -->

Fairness imposes constraints, limits what models can be learned

**But:** Arguably, unfair predictions are not desirable!

Determine how much compromise in accuracy or fairness is acceptable to
  your stakeholders

<!-- colend -->

<!-- references_ -->

[Fairness Constraints: Mechanisms for Fair Classification.](https://proceedings.mlr.press/v54/zafar17a.html) Zafar et
al. (AISTATS 2017).
</script></section><section data-markdown><script type="text/template">## Fairness, Accuracy, and Profits

![](loanprofit.png)
<!-- .element: class="stretch" -->

<!-- references_ -->
Interactive visualization: https://research.google.com/bigpicture/attacking-discrimination-in-ml/
</script></section><section data-markdown><script type="text/template">## Fairness, Accuracy, and Profits

Fairness can conflict with accuracy goals

Fairness can conflict with organizational goals (profits, usability)

Fairer products may attract more customers

Unfair products may receive bad press, reputation damage

Improving fairness through better data can benefit everybody



<!-- ---- -->
<!-- ## Discussion: Fairness Goal for Cancer Prognosis? -->

<!-- ![](mri.jpg) -->
<!-- <\!-- .element: class="stretch" -\-> -->

<!-- * What kind of harm can be caused?  -->
<!-- * Fairness goal: Equality or equity? -->
<!-- * Model: Anti-classification, group fairness, or equalized odds (with FPR/FNR)? -->
</script></section><section data-markdown><script type="text/template">## Example: Recidivism Revisited

![](recidivism-propublica.png)
<!-- .element: class="stretch" -->

* COMPAS system, developed by Northpointe: Used by judges in
  sentencing decisions across multiple states (incl. PA)
</div>

<!-- references_ -->

[ProPublica article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

</script></section><section data-markdown><script type="text/template">## Which fairness definition?

![](compas-metrics.png)
<!-- .element: class="stretch plain" -->

* ProPublica: COMPAS violates equalized odds w/ FPR & FNR
* Northpointe: COMPAS is fair because it has similar FDRs
  * FDR = FP / (FP + TP) = 1 - Precision; FPR = FP / (FP + TN)
* __Q. Which measure is appropriate in this context?__

<!-- references_ -->
[Figure from Big Data and Social Science, Ch. 11](https://textbook.coleridgeinitiative.org/chap-bias.html#ref-angwin2016b)
A. Chouldechova [Fair prediction with disparate impact: A study of bias in recidivism prediction instruments](https://arxiv.org/pdf/1703.00056.pdf)

<aside class="notes"><p>False discovery rate: rate of Type 1 errors
(reject null hypothesis when it&#39;s true)</p>
<p>Positive predictive value is the probability that a patient with a positive
(abnormal) test result actually has the disease. Negative predictive value is
the probability that a person with a negative (normal) test result is truly free
of disease.</p>
<p>when the positive predictive values are constrained to be equal but the
prevalences differ across groups, the false positive and false negative rates
cannot both be equal across those groups.</p>
<p>(i) Allow unequal false negative rates to retain equal PPV‚Äôs and achieve equal false positive rates
(ii) Allow unequal false positive rates to retain equal PPV‚Äôs and achieve equal false negative rates
(iii) Allow unequal PPV‚Äôs to achieve equal false positive and false negative rates</p>
</aside></script></section><section data-markdown><script type="text/template">## Fairness Tree

![](fairness_tree.png)
<!-- .element: class="stretch plain" -->

<!-- references_ -->

Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter and Julia Lane. [Big Data and Social Science: Data Science Methods and Tools for Research and Practice](https://textbook.coleridgeinitiative.org/). Chapter 11, 2nd ed, 2020



</script></section><section data-markdown><script type="text/template">## Breakout: Fairness Goal for Hire Decisions?

![](hiring.png)
<!-- .element: class="stretch" -->

Post as a group in #lecture: 
* What kind of harm can be caused (to different stakeholders)? 
* Fairness goal: Equality or equity?
* Model: Anti-classification, group fairness, or equalized odds (with FPR/FNR)?
</script></section><section data-markdown><script type="text/template">## Law: "Four-fifth rule" (or "80% rule")

<div class="smallish">

* Group fairness with a threshold: $\frac{P[R = 1 | A = a]}{P[R = 1 | A = b]} \geq 0.8$
* Selection rate for a protected group (e.g., $A = a$) <
80% of highest rate => selection procedure considered as having "adverse
impact"
* Guideline adopted by Federal agencies (Department of Justice, Equal
Employment Opportunity Commission, etc.,) in 1978
* If violated, must justify business necessity (i.e., the selection procedure is
essential to the safe & efficient operation)
* Example: Hiring 50% of male applicants vs 20% female applicants hired
  (0.2/0.5 = 0.4) -- Is there a business justification for hiring men at a higher rate?

</div>

<aside class="notes"><p>skip me</p>
</aside></script></section></section><section ><section data-markdown><script type="text/template"># Dataset Construction for Fairness

<!-- ![](fairness-accuracy.jpeg) -->
![](bongo.gif)

* Instead of just focusing on building a "fair"' model, can we understand &
  address the root causes of bias?
  
<!-- references_ -->

</script></section><section data-markdown><script type="text/template">## Flexibility in Data Collection

* Data science education often assumes data as given
* In industry, we often have control over data collection and curation (65%)
* Most address fairness issues by collecting more data (73%)
  * Carefully review data collection procedures, sampling bias, how
  trustworthy labels are
  * **Often high-leverage point to improve fairness!**


<!-- references -->

[Challenges of incorporating algorithmic fairness into practice](https://www.youtube.com/watch?v=UicKZv93SOY),
FAT* Tutorial, 2019  ([slides](https://bit.ly/2UaOmTG))
</script></section><section data-markdown><script type="text/template">## Data Bias

![](data-bias-stage.png)
<!-- .element: class="stretch" -->

* Bias can be introduced at any stage of the data pipeline!

<!-- references_ -->

Bennett et al., [Fairness-aware Machine Learning](https://sites.google.com/view/wsdm19-fairness-tutorial), WSDM Tutorial (2019).

</script></section><section data-markdown><script type="text/template">## Types of Data Bias

* __Population bias__
* __Behavioral bias__
* Historical bias
* Content production bias
* Linking bias
* Temporal bias

<!-- references -->

_Social Data: Biases, Methodological Pitfalls, and Ethical
Boundaries_, Olteanu et al., Frontiers in Big Data (2016).
</script></section><section data-markdown><script type="text/template">## Population Bias

![](facial-dataset.png)
<!-- .element: class="stretch" -->

* Differences in demographics between dataset vs target population
* May result in degraded services for certain groups 

<!-- references_ -->
Merler, Ratha, Feris, and Smith. [Diversity in Faces](https://arxiv.org/abs/1901.10436)
</script></section><section data-markdown><script type="text/template">## Behavioral Bias

![](freelancing.png)
<!-- .element: class="stretch" -->

* Differences in user behavior across platforms or social contexts
* Example: Freelancing platforms (Fiverr vs TaskRabbit)
  * Bias against certain minority groups on different platforms

<!-- references_ -->

_Bias in Online Freelance Marketplaces_, Hannak et al., CSCW (2017).
</script></section><section data-markdown><script type="text/template">## Fairness-Aware Data Collection

* Address population bias
<!-- .element: class="fragment" -->
  * Does the dataset reflect the demographics in the target
  population?
  * If not, collect more data to achieve this
* Address under- & over-representation issues
<!-- .element: class="fragment" -->
	* Ensure sufficient amount of data for all groups to avoid being
	treated as "outliers" by ML
	* Also avoid over-representation of certain groups (e.g.,
     remove historical data)
	
<!-- references_ -->
_Fairness-aware Machine Learning_, Bennett et al., WSDM Tutorial (2019).
</script></section><section data-markdown><script type="text/template">## Fairness-Aware Data Collection

* Data augmentation: Synthesize data for minority groups to reduce under-representation
  <!-- .element: class="fragment" -->
  * Observed: "He is a doctor" -> synthesize "She is a doctor"
* Model auditing for better data collection
  <!-- .element: class="fragment" -->
  * Evaluate accuracy across different groups
  * Collect more data for groups with highest error rates 
	
<!-- references_ -->
_Fairness-aware Machine Learning_, Bennett et al., WSDM Tutorial (2019).
</script></section><section data-markdown><script type="text/template">## Example Audit Tool: Aequitas

![](aequitas.png)
</script></section><section data-markdown><script type="text/template">## Example Audit Tool: Aequitas

![](aequitas-report.png)
<!-- .element: class="stretch" -->


<!-- ---- -->
<!-- ## Breaekout: College Admission -->

<!-- ![](college-admission.jpg) -->
<!-- <\!-- .element: class="stretch" -\-> -->

<!-- * Features: GPA, GRE/SAT, gender, race, undergrad institute, alumni -->
<!--   connections, household income, hometown, etc.,  -->
<!-- * Type into #lecture in Slack: -->
<!--   * What are different sub-populations? -->
<!--   * What are potential sources of data bias? -->
<!--   * What are ways to mitigate this bias? -->
</script></section><section data-markdown><script type="text/template">## Documentation for Fairness: Data Sheets

![](datasheet.png)
<!-- .element: class="stretch" -->

* Common practice in the electronics industry, medicine
* Purpose, provenance, creation, __composition__, distribution
  * "Does the dataset relate to people?"
  * "Does the dataset identify any subpopulations (e.g., by age)?"

<!-- references_ -->

_Datasheets for Dataset_, Gebru et al., (2019). https://arxiv.org/abs/1803.09010

<aside class="notes"><p>In the electronics industry, every component, no matter how simple or complex,
is accompanied with a datasheet that describes its operating characteristics,
test results, recommended uses, and other information. By analogy, we propose
that every dataset be accompanied with a datasheet that documents its
motivation, composition, collection process, recommended uses, and so on.
Datasheets for datasets will facilitate better communication between dataset
creators and dataset consumers, and encourage the machine learning community to
prioritize transparency and accountability.</p>
</aside></script></section><section data-markdown><script type="text/template">## Model Cards

See also: https://modelcards.withgoogle.com/about

![Model Card Example](modelcards.png)
<!-- .element: class="stretch" -->

<!-- references_ -->

Mitchell, Margaret, et al. "[Model cards for model reporting](https://arxiv.org/abs/1810.03993)." In Proceedings of the Conference on fairness, accountability, and transparency, pp. 220-229. 2019.

<aside class="notes"><p>Model cards are short documents accompanying trained machine learning models
that provide benchmarked evaluation in a variety of conditions, such as across
different cultural, demographic, or phenotypic groups (e.g., race, geographic
location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and
race, or sex and Fitzpatrick skin type) that are relevant to the intended
application domains. Model cards also disclose the context in which models are
intended to be used, details of the performance evaluation procedures, and other
relevant information. While we focus primarily on human-centered machine
learning models in the application fields of computer vision and natural
language processing, this framework can be used to document any trained machine
learning model.</p>
</aside></script></section><section data-markdown><script type="text/template">## Dataset Exploration

![](what-if-tool.png)
<!-- .element: class="stretch" -->

[Google What-If Tool](https://pair-code.github.io/what-if-tool/demos/compas.html)

<!-- ---- -->
<!-- ## Breakout: Data Collection for Fairness -->


<!-- * For each system, discuss: -->
<!--   * What harms can be caused by this system? -->
<!--   * What are possible types of bias in the data? -->
<!-- 	* Population bias? Under- or over-representation? -->
<!--   * How would you modify the dataset reduce bias? -->
<!--   * Collect more data? Remove? Augment? -->





</script></section></section><section ><section data-markdown><script type="text/template"># Fairness beyond the Model
</script></section><section data-markdown><script type="text/template">## Bias Mitigation through System Design

<!-- discussion -->

Examples of mitigations around the model?
</script></section><section data-markdown><script type="text/template">## 1. Avoid Unnecessary Distinctions


![Healthcare worker applying blood pressure monitor](blood-pressure-monitor.jpg)

*Image captioning gender biased?*

</script></section><section data-markdown><script type="text/template">## 1. Avoid Unnecessary Distinctions


![Healthcare worker applying blood pressure monitor](blood-pressure-monitor.jpg)
<!-- .element: class="stretch" -->

"Doctor/nurse applying blood pressure monitor" -> "Healthcare worker applying blood pressure monitor"

</script></section><section data-markdown><script type="text/template">## 1. Avoid Unnecessary Distinctions

Is the distinction actually necessary? Is there a more general class to unify them?

Aligns with notion of *justice* to remove the problem from the system

</script></section><section data-markdown><script type="text/template">## 2. Suppress Potentially Problem Outputs

![Twitter post of user complaining about misclassification of friends as Gorilla](apes.png)
<!-- .element: class="stretch" -->

*How to fix?*

</script></section><section data-markdown><script type="text/template">## 2. Suppress Potentially Problem Outputs

<!-- colstart -->

<div class="smallish">

Anticipate problems or react to reports

Postprocessing, filtering, safeguards
* Suppress entire output classes
* Hardcoded rules or other models (e.g., toxicity detection)

May degrade system quality for some use cases

See mitigating mistakes generally

</div>

<!-- col -->

![Google cannot find Gorilla](google_gorilla.jpg)

<!-- colend -->
</script></section><section data-markdown><script type="text/template">## 3. Design Fail-Soft Strategy

Example: Plagiarism detector

<!-- colstart -->

**A: Cheating detected! This incident has been reported.**

<!-- col -->

**B: This answer seems to be perfect. Would you like another exercise?**


<!-- colend -->


HCI principle: Fail-soft interfaces avoid calling out directly; communicate friendly and constructively to allow saving face

Especially relevant if system unreliable or biased

</script></section><section data-markdown><script type="text/template">## 4. Keep Humans in the Loop


![Temi.com screenshot](temi.png)
<!-- .element: class="stretch" -->

TV subtitles: Humans check transcripts, especially with heavy dialects
</script></section><section data-markdown><script type="text/template">## 4. Keep Humans in the Loop

Recall: Automate vs prompt vs augment

* Involve humans to correct for mistakes and bias
* But, model often introduced to avoid bias in human decision
* But, challenging human-interaction design to keep humans engaged and alert; human monitors possibly biased too, making it worse

**Does a human have a fair chance to detect and correct bias?** Enough information? Enough context? Enough time? Unbiased human decision?
</script></section><section data-markdown><script type="text/template">## Predictive Policing Example

> "officers expressed skepticism
about the software and during ride alongs showed no intention of using it"

> "the officer discounted the software since it showed what he already
knew, while he ignored those predictions that he did not understand"

Does the system just lend credibility to a biased human process?

<!-- references -->
Lally, Nick. "[‚ÄúIt makes almost no difference which algorithm you use‚Äù: on the modularity of predictive policing](http://www.nicklally.com/wp-content/uploads/2016/09/lallyModularityPP.pdf)." Urban Geography (2021): 1-19.

</script></section><section data-markdown><script type="text/template">## (More positive) Medical example

<div class="smallish">

> Mid-level clinicians often doubted that their voice was heard or
that their expertise was considered. They were hesitant to
directly disagree with a physician. They described the situation as more complicated than just the power dynamics...they had
slowly removed themselves from the decision making.

> "A prognostic DST that indicates post-surgery quality of life
could potentially amplify their voices: 
"...I think it‚Äôs
good to have something visual for anybody to see. It‚Äôs
like, OK. LOOK. Let‚Äôs slow down a bit here." (Nurse
practitioner)"


<!-- references -->
Qian, Yang, et al. "[Unremarkable AI: Fiting Intelligent Decision Support
into Critical, Clinical Decision-Making Processes](https://dl.acm.org/doi/pdf/10.1145/3290605.3300468)." CSCW 2019.

</div>





</script></section></section><section ><section data-markdown><script type="text/template"># Monitoring

</script></section><section data-markdown><script type="text/template">## Monitoring & Auditing

* Operationalize fairness measure in production with telemetry
* Continuously monitor for:
<!-- .element: class="fragment" -->
  - Mismatch between training data, test data, and instances encountered in deployment
  - Data shifts: May suggest needs to adjust fairness metric/thresholds
  - User reports & complaints: Log and audit system decisions
    perceived to be unfair by users
* Invite diverse stakeholders to audit system for biases
<!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">## Monitoring & Auditing

![](model_drift.jpg)
<!-- .element: class="stretch" -->

* Continuosly monitor the fairness metric (e.g., error rates for
different sub-populations)
* Re-train model with recent data or adjust classification thresholds
  if needed

</script></section><section data-markdown><script type="text/template">## Preparing for Problems

Prepare an *incidence response plan* for fairness issues
* What can be shut down/reverted on short notice?
* Who does what?
* Who talks to the press? To affected parties? What do they need to know?

Provide users with a path to *appeal decisions*
* Provide feedback mechanism to complain about unfairness
* Human review? Human override?




</script></section></section><section ><section data-markdown><script type="text/template"># Process Integration
</script></section><section data-markdown><script type="text/template">## Fairness in Practice today

Lots of attention in academia and media

Lofty statements by big companies, mostly aspirational

Strong push by few invested engineers (internal activists)

Some dedicated teams, mostly in Big Tech, mostly research focused

Little institutional support, no broad practices
</script></section><section data-markdown><script type="text/template">## Barriers to Fairness Work

<!-- discussion -->


</script></section><section data-markdown><script type="text/template">## Barriers to Fairness Work

<div class="smallish">

1. Rarely an organizational priority, mostly reactive (media pressure, regulators)
2. Fairness work seen as ambiguous and too complicated for available resources (esp. outside Big Tech)
3. Most fairness work done by volunteers outside official job functions
4. Impact of fairness work difficult to quantify, making it hard to justify resource investment
5. Technical challenges
6. Fairness concerns are project specific, hard to transfer actionable insights and tools across teams


</div>
</script></section><section data-markdown><script type="text/template">## Burnout is a Real Danger

Unsupported fairness work is frustrating and often ineffective

> ‚ÄúHowever famous the company is, it‚Äôs not worth being in a work situation where you don‚Äôt feel like your entire company, or at least a significant part of your company, is trying to do this with you. Your job is not to be paid lots of money to point out problems. Your job is to help them make their product better. And if you don‚Äôt believe in the product, then don‚Äôt work there.‚Äù -- Rumman Chowdhury via [Melissa Heikkil√§](https://www.technologyreview.com/2022/11/01/1062474/how-to-survive-as-an-ai-ethicist/)

*What to do?*


</script></section><section data-markdown><script type="text/template">## Aspirations

<div class="smallish"> 

> "They imagined that organizational leadership would understand, support, and engage deeply with responsible AI concerns, which would be contextualized within their organizational context. Responsible AI would be prioritized as part of the high-level organizational mission and then translated into actionable goals down at the individual levels through established processes. Respondents wanted the spread of information to go through well-established channels so that people know where to look and how to share information."

</div>

<!-- references -->
From Rakova, Bogdana, Jingying Yang, Henriette Cramer, and Rumman Chowdhury. "Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices." Proceedings of the ACM on Human-Computer Interaction 5, no. CSCW1 (2021): 1-23.

</script></section><section data-markdown><script type="text/template">## We want to improve Process Integration

* Integrate proactive practices in development processes -- both model and system level!
* Move from individuals to institutional processes distributing the work
* Hold the entire organization accountable for taking fairness seriously

*How?*


</script></section><section data-markdown><script type="text/template">## Improving Process Integration -- Examples

1. Mandatory discussion of discrimination risks, protected attributes, and fairness goals in *requirements documents*
2. Required fairness reporting in addition to accuracy in automated *model evaluation*
3. Required internal/external fairness audit before *release*
4. Required fairness monitoring, oversight infrastructure in *operation*
</script></section><section data-markdown><script type="text/template">## Improving Process Integration -- Examples

5. Instituting fairness measures as *key performance indicators* of products
6. Assign clear responsibilities of who does what
7. Identify measurable fairness improvements, recognize in performance evaluations

*How to avoid pushback against bureaucracy?*
</script></section><section data-markdown><script type="text/template">## Affect Culture Change

<div class="smallish">

Buy-in from management is crucial
1. Show that fairness work is taken seriously through action (funding, hiring, audits, policies), not just lofty mission statements

Reported success strategies:
1. Frame fairness work as financial profitable, avoiding rework and reputation cost
2. Demonstrate concrete, quantified evidence of benefits of fairness work
3. Continuous internal activism and education initiatives
4. External pressure from customers and regulators

</div>
</script></section><section data-markdown><script type="text/template">## Assigning Responsibilities

Hire/educate T-shaped professionals

Have dedicated fairness expert(s) consulting with teams, performing/guiding audits, etc

Not everybody will be a fairness expert, but ensure base-level awareness on when to seek help




</script></section></section><section ><section data-markdown><script type="text/template"># Best Practices
</script></section><section data-markdown><script type="text/template">## Best Practices

**Best practices are emerging and evolving**

Start early, be proactive

Scrutinize data collection and labeling

Invest in requirements engineering and design

Invest in education

Assign clear responsibilities, demonstrate leadership buy-in
</script></section><section data-markdown><script type="text/template">## Many Tutorials, Checklists, Recommendations

Tutorials (fairness notions, sources of bias, process recom.): 
* [Fairness in Machine Learning](https://vimeo.com/248490141), [Fairness-Aware Machine Learning in Practice](https://sites.google.com/view/fairness-tutorial)
* [Challenges of Incorporating Algorithmic Fairness into Industry Practice](https://www.microsoft.com/en-us/research/video/fat-2019-translation-tutorial-challenges-of-incorporating-algorithmic-fairness-into-industry-practice/)

Checklist:
* Microsoft‚Äôs [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/): concrete questions, concrete steps throughout all stages, including deployment and monitoring




</script></section></section><section ><section data-markdown><script type="text/template"># Anticipate Feedback Loops
</script></section><section data-markdown><script type="text/template">## Feedback Loops

![Feedback loop](feedbackloop.svg)
<!-- .element: class="plain" -->
</script></section><section data-markdown><script type="text/template">## Feedback Loops in Mortgage Applications?

<!-- discussion -->
</script></section><section data-markdown><script type="text/template">## Feedback Loops go through the Environment

![](component.svg)
<!-- .element: class="plain" -->

</script></section><section data-markdown><script type="text/template">## Long-term Impact of ML

* ML systems make multiple decisions over time, influence the
behaviors of populations in the real world
* *But* most models are built & optimized assuming that the world is
static
* Difficult to estimate the impact of ML over time
  * Need to reason about the system dynamics (world vs machine)
  * e.g., what's the effect of a mortgage lending policy on a population?
</script></section><section data-markdown><script type="text/template">## Long-term Impact & Fairness

<!-- colstart -->

Deploying an ML model with a fairness criterion does NOT guarantee
  improvement in equality/equity over time

Even if a model appears to promote fairness in
short term, it may result harm over long term 

<!-- col -->

![](fairness-longterm.png)
<!-- .element: class="stretch" -->

<!-- colend -->

<!-- references_ -->
[Fairness is not static: deeper understanding of long term fairness via simulation studies](https://dl.acm.org/doi/abs/10.1145/3351095.3372878),
in FAT* 2020.
1
</script></section><section data-markdown><script type="text/template">## Analyze the World vs the Machine

![world vs machine](worldvsmachine.svg)
<!-- .element: class="plain stretch" -->

*State and check assumptions!*

</script></section><section data-markdown><script type="text/template">## Analyze the World vs the Machine

How do outputs affect change in the real world, how does this (indirectly) influence inputs?

Can we decouple inputs from outputs? Can telemetry be trusted?

Interventions through system (re)design:
* Focus data collection on less influenced inputs
* Compensate for bias from feedback loops in ML pipeline
* Do not build the system in the first place


</script></section><section data-markdown><script type="text/template">## Prepare for Feedback Loops

We will likely not anticipate all feedback loops...

... but we can anticipate that unknown feedback loops exist

-> Monitoring!



</script></section></section><section ><section data-markdown><script type="text/template"># Summary

* Requirements engineering for fair ML systems
  * Identify potential harms, protected attributes
  * Negotiate conflicting fairness goals, tradeoffs
  * Consider societal implications
* Apply fair data collection practices 
* Anticipate feedback loops
* Operationalize & monitor for fairness metrics  
* Design fair systems beyond the model, mitigate bias outside the model
* Integrate fairness work in process and culture

</script></section><section data-markdown><script type="text/template">## Further Readings

<div class="smaller">

- üóé Rakova, Bogdana, Jingying Yang, Henriette Cramer, and Rumman Chowdhury. "[Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices](https://arxiv.org/abs/2006.12358)." *Proceedings of the ACM on Human-Computer Interaction* 5, no. CSCW1 (2021): 1-23.
- üóé Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. "[Model cards for model reporting](https://arxiv.org/abs/1810.03993)." In *Proceedings of the conference on fairness, accountability, and transparency*, pp. 220-229. 2019.
- üóé Boyd, Karen L. "[Datasheets for Datasets help ML Engineers Notice and Understand Ethical Issues in Training Data](http://karenboyd.org/Datasheets_Help_CSCW.pdf)." Proceedings of the ACM on Human-Computer Interaction 5, no. CSCW2 (2021): 1-27.
- üóé Bietti, Elettra. "[From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy](https://dl.acm.org/doi/pdf/10.1145/3351095.3372860)." In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 210-219. 2020.
- üóé Madaio, Michael A., Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. "[Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI](http://www.jennwv.com/papers/checklists.pdf)." In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1-14. 2020.
- üóé Hopkins, Aspen, and Serena Booth. "[Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development](http://www.slbooth.com/papers/AIES-2021_Hopkins_and_Booth.pdf)." In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (AIES ‚Äô21) (2021).
- üóé Metcalf, Jacob, and Emanuel Moss. "[Owning ethics: Corporate logics, silicon valley, and the institutionalization of ethics](https://datasociety.net/wp-content/uploads/2019/09/Owning-Ethics-PDF-version-2.pdf)." *Social Research: An International Quarterly* 86, no. 2 (2019): 449-476.

</div>
</script></section></section></div>
    </div>

    <script src="./../dist/reveal.js"></script>

    <script src="./../_assets/mymarkdown.js"></script>
    <script src="./../plugin/markdown/markdown.js"></script>
    <script src="./../plugin/highlight/highlight.js"></script>
    <script src="./../plugin/zoom/zoom.js"></script>
    <script src="./../plugin/notes/notes.js"></script>
    <script src="./../plugin/math/math.js"></script>

    <script src="./../node_modules/reveal.js-menu/menu.js"></script>
    <script src="./../node_modules/reveal.js-plugins/embed-tweet/plugin.js"></script>


    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        controls: true,
        slideNumber: true,
        markdown: {
          renderer: patchMarkdown(RevealMarkdown().marked)
        },
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath,
          RevealMenu,
          RevealEmbedTweet
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"controls":true,"progress":true,"theme":"white","slideNumber":true,"hash":true,"history":false,"center":false,"width":1280,"height":720,"margin":0.1,"transition":"none"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>